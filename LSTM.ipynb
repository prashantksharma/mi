{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Networks\n",
    "\n",
    "Numpy implementation of mnist handwritten digit recognition with LSTM. This code is based on [wiseodd's github.io](http://wiseodd.github.io/techblog/2016/08/12/lstm-backprop/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Prepare mnist, weights and delta values to use in training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 28\n",
    "HIDDEN = 128\n",
    "OUTPUT = 10\n",
    "\n",
    "INPUT += HIDDEN\n",
    "\n",
    "ALPHA = 0.001\n",
    "BATCH_NUM = 64\n",
    "\n",
    "ITER_NUM = 1000\n",
    "LOG_ITER = ITER_NUM // 10\n",
    "PLOT_ITER = ITER_NUM // 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [] # to plot learning curve of cross entropy\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "wf = np.random.randn(INPUT, HIDDEN) / np.sqrt(INPUT / 2)\n",
    "wi = np.random.randn(INPUT, HIDDEN) / np.sqrt(INPUT / 2)\n",
    "wc = np.random.randn(INPUT, HIDDEN) / np.sqrt(INPUT / 2)\n",
    "wo = np.random.randn(INPUT, HIDDEN) / np.sqrt(INPUT / 2)\n",
    "wy = np.random.randn(HIDDEN, OUTPUT) / np.sqrt(HIDDEN / 2)\n",
    "\n",
    "bf = np.zeros(HIDDEN)\n",
    "bi = np.zeros(HIDDEN)\n",
    "bc = np.zeros(HIDDEN)\n",
    "bo = np.zeros(HIDDEN)\n",
    "by = np.zeros(OUTPUT)\n",
    "\n",
    "dwf = np.zeros_like(wf)\n",
    "dwi = np.zeros_like(wi)\n",
    "dwc = np.zeros_like(wc)\n",
    "dwo = np.zeros_like(wo)\n",
    "dwy = np.zeros_like(wy)\n",
    "\n",
    "dbf = np.zeros_like(bf)\n",
    "dbi = np.zeros_like(bi)\n",
    "dbc = np.zeros_like(bc)\n",
    "dbo = np.zeros_like(bo)\n",
    "dby = np.zeros_like(by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition\n",
    "\n",
    "Define activation functions and LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr):\n",
    "    c = np.clip(arr, -700, 700) # float64 maximum expotentiable value\n",
    "    e = np.exp(c)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(out, label):\n",
    "    entropy = label * np.log(out + 1e-6) # to prevent log value overflow\n",
    "    return -np.sum(entropy, axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid(arr):\n",
    "    c = np.clip(arr, -700, 700)\n",
    "    return 1 / (1 + np.exp(-c))\n",
    "\n",
    "def deriv_sigmoid(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "def tanh(arr):\n",
    "    c = np.clip(arr, -350, 350)\n",
    "    return 2 / (1 + np.exp(-2 * c)) - 1\n",
    "\n",
    "def deriv_tanh(out):\n",
    "    return 1 - np.square(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Cell(input_val):\n",
    "    batch_num = input_val.shape[1]\n",
    "    \n",
    "    caches = []\n",
    "    states = []\n",
    "    states.append([np.zeros([batch_num, HIDDEN]), np.zeros([batch_num, HIDDEN])])\n",
    "    \n",
    "    for x in input_val:\n",
    "        c_prev, h_prev = states[-1]\n",
    "    \n",
    "        x = np.column_stack([x, h_prev])\n",
    "        hf = sigmoid(np.dot(x, wf) + bf)\n",
    "        hi = sigmoid(np.dot(x, wi) + bi)\n",
    "        ho = sigmoid(np.dot(x, wo) + bo)\n",
    "        hc = tanh(np.dot(x, wc) + bc)\n",
    "    \n",
    "        c = hf * c_prev + hi * hc\n",
    "        h = ho * tanh(c)\n",
    "    \n",
    "        states.append([c, h])\n",
    "        caches.append([x, hf, hi, ho, hc])\n",
    "        \n",
    "    return caches, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    input_val = np.reshape(img, [28, 1, 28])\n",
    "    \n",
    "    caches, states = LSTM_Cell(input_val)\n",
    "    c, h = states[-1]\n",
    "    \n",
    "    pred = softmax(np.dot(h, wy) + by)\n",
    "    label = np.argmax(pred)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train the model of mnist handwritten digit recognition using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ITER_NUM+1):\n",
    "    X, Y = mnist.train.next_batch(BATCH_NUM)\n",
    "    Xt = np.transpose(np.reshape(X, [-1, 28, 28]), [1, 0, 2])\n",
    "        \n",
    "    caches, states = LSTM_Cell(Xt)\n",
    "    c, h = states[-1]\n",
    "        \n",
    "    out = np.dot(h, wy) + by\n",
    "    pred = softmax(out)\n",
    "    entropy = cross_entropy(pred, Y)\n",
    "    \n",
    "    # Backpropagation Through Time\n",
    "    dout = pred - Y\n",
    "    dwy = np.dot(h.T, dout)\n",
    "    dby = np.sum(dout, axis=0)\n",
    "    \n",
    "    dc_next = np.zeros_like(c)\n",
    "    dh_next = np.zeros_like(h)\n",
    "    \n",
    "    for t in range(Xt.shape[0]):\n",
    "        c, h = states[-t-1]\n",
    "        c_prev, h_prev = states[-t-2]\n",
    "\n",
    "        x, hf, hi, ho, hc = caches[-t-1]\n",
    "        \n",
    "        tc = tanh(c)\n",
    "        dh = np.dot(dout, wy.T) + dh_next\n",
    "        \n",
    "        dc = dh * ho * deriv_tanh(tc)\n",
    "        dc = dc + dc_next\n",
    "        \n",
    "        dho = dh * tc \n",
    "        dho = dho * deriv_sigmoid(ho)\n",
    "        \n",
    "        dhf = dc * c_prev \n",
    "        dhf = dhf * deriv_sigmoid(hf)\n",
    "        \n",
    "        dhi = dc * hc \n",
    "        dhi = dhi * deriv_sigmoid(hi)\n",
    "        \n",
    "        dhc = dc * hi \n",
    "        dhc = dhc * deriv_tanh(hc)\n",
    "        \n",
    "        dwf += np.dot(x.T, dhf)\n",
    "        dbf += np.sum(dhf, axis=0)\n",
    "        dXf = np.dot(dhf, wf.T)\n",
    "        \n",
    "        dwi += np.dot(x.T, dhi)\n",
    "        dbi += np.sum(dhi, axis=0)\n",
    "        dXi = np.dot(dhi, wi.T)\n",
    "        \n",
    "        dwo += np.dot(x.T, dho)\n",
    "        dbo += np.sum(dho, axis=0)\n",
    "        dXo = np.dot(dho, wo.T)\n",
    "        \n",
    "        dwc += np.dot(x.T, dhc)\n",
    "        dbc += np.sum(dhc, axis=0)\n",
    "        dXc = np.dot(dhc, wc.T)\n",
    "\n",
    "        dX = dXf + dXi + dXo + dXc\n",
    "        \n",
    "        dc_next = hf * dc\n",
    "        dh_next = dX[:, -HIDDEN:]\n",
    "        \n",
    "    # Update weights\n",
    "    wf -= ALPHA * dwf\n",
    "    wi -= ALPHA * dwi\n",
    "    wc -= ALPHA * dwc\n",
    "    wo -= ALPHA * dwo\n",
    "    wy -= ALPHA * dwy\n",
    "    \n",
    "    bf -= ALPHA * dbf\n",
    "    bi -= ALPHA * dbi\n",
    "    bc -= ALPHA * dbc\n",
    "    bo -= ALPHA * dbo\n",
    "    by -= ALPHA * dby\n",
    "    \n",
    "    # Initialize delta values\n",
    "    dwf *= 0\n",
    "    dwi *= 0\n",
    "    dwc *= 0\n",
    "    dwo *= 0\n",
    "    dwy *= 0\n",
    "    \n",
    "    dbf *= 0\n",
    "    dbi *= 0\n",
    "    dbc *= 0\n",
    "    dbo *= 0\n",
    "    dby *= 0\n",
    "    \n",
    "    # Log training data\n",
    "    if i % PLOT_ITER == 0:\n",
    "        errors.append(np.sum(entropy))\n",
    "    \n",
    "    if i % LOG_ITER == 0:\n",
    "        print('iter', i)\n",
    "        print('entropy', np.sum(entropy))\n",
    "        print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Result\n",
    "\n",
    "Plot a learning curve of the cross entropy loss. Also test prediction of new img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(55000)\n",
    "img = mnist.train.images[i]\n",
    "img = np.reshape(img, [28, 28])\n",
    "\n",
    "pred = predict(img)\n",
    "print('prediction :', pred)\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
